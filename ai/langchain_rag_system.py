#!/usr/bin/env python3
"""
LangChain RAG System with Tools
Uses LangChain tools to search channels incrementally
"""

import os
import json
import re
from typing import List, Dict, Any, Tuple, Optional
import openai
from dotenv import load_dotenv
from datetime import datetime
import time

# Load environment variables
load_dotenv()

# Configure OpenAI client
openai.api_key = os.getenv('GEMINI_API_KEY')
openai.base_url = os.getenv('GEMINI_BASE_URL', 'https://api.gapgpt.app/v1')

class LangChainRAGSystem:
    def __init__(self, database_file: str = "test_channels_database.json", config_file: str = "multi_channel_config.json"):
        self.database_file = database_file
        self.config_file = config_file
        self.database = self.load_database()
        self.config = self.load_config()
        self.active_channels = [ch for ch in self.config['channels'] if ch['active']]
        self.embedding_cache = {}  # Cache for embeddings
        
        # Initialize LangChain components
        self.initialize_langchain()
        
    def initialize_langchain(self):
        """Initialize LangChain components"""
        try:
            from langchain_openai import ChatOpenAI
            from langchain.tools import Tool
            from langchain.agents import AgentExecutor, create_openai_tools_agent
            from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
            from sentence_transformers import SentenceTransformer
            
            # Initialize sentence transformer for semantic search - using faster model
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # Initialize LLM
            self.llm = ChatOpenAI(
                model="gemini-2.5-flash",
                temperature=0.7,
                api_key=openai.api_key,
                base_url=openai.base_url
            )
            
            # Create tools
            self.tools = self.create_tools()
            
            # Create agent
            self.agent = self.create_agent()
            
            print("âœ… LangChain system initialized successfully")
            
        except Exception as e:
            print(f"âŒ Error initializing LangChain: {e}")
            print("âš ï¸ Falling back to basic search")
            self.llm = None
            self.agent = None
    
    def create_tools(self) -> List:
        """Create LangChain tools for searching channels"""
        from langchain.tools import StructuredTool
        from pydantic import BaseModel, Field
        
        class SearchChannelsInput(BaseModel):
            query: str = Field(description="Query to search for relevant channels")
        
        class SearchMessagesInput(BaseModel):
            channel_username: str = Field(description="Username of the channel to search in")
            query: str = Field(description="Query to search for in the channel")
        
        class GetChannelInfoInput(BaseModel):
            channel_username: str = Field(description="Username of the channel to get info for")
        
        class ExpandSearchInput(BaseModel):
            current_channels: str = Field(description="Comma-separated list of current channels")
            query: str = Field(description="Query to expand search for")
        
        tools = [
            StructuredTool.from_function(
                func=self.search_relevant_channels,
                name="search_channels",
                description="Search for relevant channels based on a query. Returns list of channel usernames.",
                args_schema=SearchChannelsInput
            ),
            StructuredTool.from_function(
                func=self.search_messages_in_channel,
                name="search_messages_in_channel",
                description="Search for messages in a specific channel. Returns relevant messages.",
                args_schema=SearchMessagesInput
            ),
            StructuredTool.from_function(
                func=self.get_channel_info,
                name="get_channel_info",
                description="Get information about a specific channel. Returns channel details.",
                args_schema=GetChannelInfoInput
            ),
            StructuredTool.from_function(
                func=self.expand_search,
                name="expand_search",
                description="Expand search to more channels if current results are insufficient. Returns additional channels.",
                args_schema=ExpandSearchInput
            )
        ]
        
        return tools
    
    def create_agent(self):
        """Create LangChain agent"""
        from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
        from langchain.agents import create_openai_tools_agent, AgentExecutor
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø´Ø±ÛŒÙ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯.

**Ø§ØµÙˆÙ„ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ:**
- Ù‡Ù…ÛŒØ´Ù‡ Ø¨ÛŒâ€ŒØ·Ø±ÙØŒ Ø¹ÛŒÙ†ÛŒ Ùˆ Ø¹Ù„Ù…ÛŒ Ø¨Ø§Ø´ÛŒØ¯
- ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯
- Ø§Ø² Ø§Ø¸Ù‡Ø§Ø± Ù†Ø¸Ø± Ø´Ø®ØµÛŒ ÛŒØ§ Ø¬Ø§Ù†Ø¨â€ŒØ¯Ø§Ø±ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†ÛŒØ¯
- Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø§Ù…Ø¹ Ùˆ Ù…Ù†Ø¸Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯

**Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¬Ø³ØªØ¬Ùˆ:**
1. Ø§Ø¨ØªØ¯Ø§ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¯Ø§Ø± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯: sharifdaily Ùˆ sharif_senfi
2. Ø³Ù¾Ø³ Ø­Ø¯Ø§Ù‚Ù„ 2-3 Ú©Ø§Ù†Ø§Ù„ Ø¯ÛŒÚ¯Ø± Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†ÛŒØ¯
3. Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†ÛŒØ³ØªØŒ Ø¨Ø§ expand_search Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
4. Ù‡Ù…ÛŒØ´Ù‡ Ø­Ø¯Ø§Ù‚Ù„ 3 Ú©Ø§Ù†Ø§Ù„ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯

**ÙØ±Ù…Øª Ù¾Ø§Ø³Ø®:**
Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ø§ÛŒÙ† Ø´Ú©Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:

**Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ** (Ù†Ø§Ù… Ø´Ø®ØµØŒ Ù…ÙˆØ¶ÙˆØ¹ ÛŒØ§ Ø±ÙˆÛŒØ¯Ø§Ø¯)

Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø´Ø±ÛŒÙ:

**Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·:**
â€¢ Ù†Ú©ØªÙ‡ Ø§ÙˆÙ„
â€¢ Ù†Ú©ØªÙ‡ Ø¯ÙˆÙ…
â€¢ Ù†Ú©ØªÙ‡ Ø³ÙˆÙ…

**Ù…Ø³Ø§Ø¦Ù„ Ù…Ø±ØªØ¨Ø·:**
â€¢ Ù…ÙˆØ¶ÙˆØ¹ Ø§ÙˆÙ„
â€¢ Ù…ÙˆØ¶ÙˆØ¹ Ø¯ÙˆÙ…

**Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø±ØªØ¨Ø·:**
1. [Ù†Ø§Ù… Ú©Ø§Ù†Ø§Ù„](https://t.me/channel_username/message_id)
2. [Ù†Ø§Ù… Ú©Ø§Ù†Ø§Ù„](https://t.me/channel_username/message_id)

**Ù†Ú©Ø§Øª Ù…Ù‡Ù…:**
- Ø§Ø² Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ÛŒ 0.6 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø­Ø¯Ø§Ú©Ø«Ø± 15 Ù¾ÛŒØ§Ù… Ø¨Ø±ØªØ± Ø§Ø² Ù‡Ø± Ú©Ø§Ù†Ø§Ù„ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯
- Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª https://t.me/channel_username/message_id Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
- Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ Ú©Ø§Ù…Ù„ Ø¨Ø¯Ù‡ÛŒØ¯
- Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø·Ù‚ÛŒ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯

**Ù„Ø­Ù† Ùˆ Ø³Ø¨Ú©:**
- Ø§Ø² Ù„Ø­Ù† Ø¹Ù„Ù…ÛŒ Ùˆ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ù†Ø¸Ù… Ùˆ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
- Ø§Ø² Ø¬Ù…Ù„Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ ÙˆØ§Ø¶Ø­ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ù‡Ù…ÛŒØ´Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯

Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯:
- search_channels: Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
- search_messages_in_channel: Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ú©Ø§Ù†Ø§Ù„
- get_channel_info: Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù†Ø§Ù„
- expand_search: Ø¨Ø±Ø§ÛŒ Ú¯Ø³ØªØ±Ø´ Ø¬Ø³ØªØ¬Ùˆ

Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯."""),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        agent = create_openai_tools_agent(self.llm, self.tools, prompt)
        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)
        
        return agent_executor
    
    def search_relevant_channels(self, query: str) -> str:
        """Search for relevant channels based on query with priority channels"""
        try:
            # Priority channels that should always be checked first
            priority_channels = ['sharifdaily', 'sharif_senfi']
            
            # Use semantic search to find relevant channels
            query_embedding = self.embedding_model.encode(query)
            
            channel_scores = []
            priority_found = []
            
            # Batch encode channel texts for better performance
            channel_texts = []
            channel_names = []
            
            for channel in self.active_channels:
                channel_text = f"{channel['name']} {channel['description']}"
                channel_texts.append(channel_text)
                channel_names.append(channel['username'])
                
                # Check if it's a priority channel
                if channel['username'] in priority_channels:
                    priority_found.append(channel['username'])
            
            # Batch encode all channel texts at once
            if channel_texts:
                channel_embeddings = self.embedding_model.encode(channel_texts)
                
                for i, username in enumerate(channel_names):
                    similarity = self.cosine_similarity(query_embedding, channel_embeddings[i])
                    channel_scores.append((username, similarity))
            
            # Sort by similarity
            channel_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Start with priority channels, then add top semantic matches
            result_channels = []
            
            # Add priority channels first (if they exist in active channels)
            for priority_ch in priority_channels:
                if priority_ch in priority_found:
                    result_channels.append(priority_ch)
            
            # Add top semantic matches (excluding priority channels already added)
            for username, score in channel_scores:
                if username not in result_channels and len(result_channels) < 8:  # Keep total around 10
                    result_channels.append(username)
            
            return f"Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· (Ø§ÙˆÙ„ÙˆÛŒØª + Ù…Ø¹Ù†Ø§ÛŒÛŒ): {', '.join(result_channels)}"
            
        except Exception as e:
            return f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§: {e}"
    

    
    def search_messages_in_channel(self, channel_username: str, query: str) -> str:
        """Search for messages in a specific channel"""
        try:
            # Handle channel name variations and fuzzy matching
            if channel_username not in self.database['channels']:
                # Try to find similar channel names
                available_channels = list(self.database['channels'].keys())
                similar_channels = []
                
                # Check for common variations
                variations = {
                    'ÛŒØ§Ø±ÛŒÚ¯Ø±Ø§Ù†': 'yarigaran_sharif',
                    'yaregaran': 'yarigaran_sharif',  # Common misspelling
                    'yareegaran': 'yarigaran_sharif',  # Another common misspelling
                    'yari': 'yarigaran_sharif',
                    'Ø´Ø±ÛŒÙ': 'sharif_senfi',
                    'Ø´ÙˆØ±Ø§ÛŒ ØµÙ†ÙÛŒ': 'sharif_senfi',
                    'Ø±ÙˆØ²Ù†Ø§Ù…Ù‡': 'sharifdaily',
                    'daily': 'sharifdaily'
                }
                
                # Check if there's a direct variation match
                if channel_username in variations:
                    suggested_channel = variations[channel_username]
                    if suggested_channel in self.database['channels']:
                        # Automatically search in the suggested channel instead of just suggesting
                        print(f"ğŸ”„ Redirecting from '{channel_username}' to '{suggested_channel}'")
                        return self.search_messages_in_channel(suggested_channel, query)
                
                # Also check for partial matches in variations
                for variation, suggested_channel in variations.items():
                    if variation in channel_username or channel_username in variation:
                        if suggested_channel in self.database['channels']:
                            print(f"ğŸ”„ Redirecting from '{channel_username}' to '{suggested_channel}' (partial match)")
                            return self.search_messages_in_channel(suggested_channel, query)
                
                # Find similar channel names
                for available in available_channels:
                    if channel_username.lower() in available.lower() or available.lower() in channel_username.lower():
                        similar_channels.append(available)
                
                # Check if there's a very close match that we should automatically redirect to
                if similar_channels:
                    # If we have a very close match, automatically redirect
                    best_match = similar_channels[0]
                    if (channel_username.lower() in best_match.lower() or 
                        best_match.lower() in channel_username.lower() or
                        any(variation in channel_username.lower() for variation in ['ÛŒØ§Ø±ÛŒÚ¯Ø±Ø§Ù†', 'yaregaran', 'yareegaran', 'yari'])):
                        print(f"ğŸ”„ Auto-redirecting from '{channel_username}' to '{best_match}'")
                        return self.search_messages_in_channel(best_match, query)
                
                # Only show error if no automatic redirect was possible
                error_msg = f"Ú©Ø§Ù†Ø§Ù„ '{channel_username}' Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª."
                
                if similar_channels:
                    error_msg += f"\n\nÚ©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ù…ÙˆØ¬ÙˆØ¯:\n"
                    for ch in similar_channels[:5]:  # Show up to 5 suggestions
                        error_msg += f"â€¢ {ch}\n"
                
                error_msg += f"\n\nÚ©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ù…ÙˆØ¬ÙˆØ¯:\n"
                main_channels = ['sharif_senfi', 'sharifdaily', 'yarigaran_sharif', 'sh_counseling']
                for ch in main_channels:
                    if ch in available_channels:
                        error_msg += f"â€¢ {ch}\n"
                
                return error_msg
            
            channel_data = self.database['channels'][channel_username]
            messages = channel_data.get('messages', [])
            
            # Search through ALL messages (no limit)
            print(f"ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ú©Ù„ {len(messages)} Ù¾ÛŒØ§Ù… Ú©Ø§Ù†Ø§Ù„ {channel_username}...")
            
            if not messages:
                return f"Ù‡ÛŒÚ† Ù¾ÛŒØ§Ù…ÛŒ Ø¯Ø± Ú©Ø§Ù†Ø§Ù„ {channel_username} ÛŒØ§ÙØª Ù†Ø´Ø¯"
            
            # Fast keyword search through ALL messages
            query_words = query.lower().split()
            message_scores = []
            
            print(f"ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø¯Ø± {len(messages)} Ù¾ÛŒØ§Ù…...")
            found_exact = 0
            found_all_words = 0
            
            for i, msg in enumerate(messages):
                if i % 2000 == 0:  # Progress indicator every 2000 messages
                    print(f"   Ù¾ÛŒØ´Ø±ÙØª: {i}/{len(messages)} ({i/len(messages)*100:.1f}%)")
                
                text = msg.get('text', '')
                if text and len(text) > 10:
                    text_lower = text.lower()
                    
                    # Check for exact phrase match first (highest priority)
                    if query.lower() in text_lower:
                        score = 0.95
                        message_scores.append((msg, score))
                        found_exact += 1
                        # Early termination if we have enough exact matches
                        if found_exact >= 20:
                            print(f"   âœ… {found_exact} ØªØ·Ø¨ÛŒÙ‚ Ø¯Ù‚ÛŒÙ‚ ÛŒØ§ÙØª Ø´Ø¯ - ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…")
                            break
                    # Check for all words present (medium priority)
                    elif all(word in text_lower for word in query_words if len(word) > 2):
                        score = 0.8
                        message_scores.append((msg, score))
                        found_all_words += 1
                        # Early termination if we have enough good matches
                        if found_all_words >= 50:
                            print(f"   âœ… {found_all_words} ØªØ·Ø¨ÛŒÙ‚ Ø®ÙˆØ¨ ÛŒØ§ÙØª Ø´Ø¯ - ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…")
                            break
                    # Check for partial matches (lower priority)
                    elif any(word in text_lower for word in query_words if len(word) > 2):
                        score = 0.6
                        message_scores.append((msg, score))
            
            # If still not enough results, do semantic search on ALL messages
            if len(message_scores) < 10:
                print("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø± Ú©Ù„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§...")
                query_embedding = self.embedding_model.encode(query)
                
                semantic_candidates = []
                for msg in messages:
                    text = msg.get('text', '')
                    if text and len(text) > 30:  # Only longer texts
                        semantic_candidates.append((msg, text[:200]))  # Limit text length
                
                if semantic_candidates:
                    print(f"   ğŸ” Ù…Ø­Ø§Ø³Ø¨Ù‡ embedding Ø¨Ø±Ø§ÛŒ {len(semantic_candidates)} Ù¾ÛŒØ§Ù…...")
                    texts_to_encode = [text for _, text in semantic_candidates]
                    embeddings = self.embedding_model.encode(texts_to_encode)
                    
                    semantic_found = 0
                    for i, (msg, _) in enumerate(semantic_candidates):
                        similarity = self.cosine_similarity(query_embedding, embeddings[i])
                        if similarity > 0.4:  # Higher threshold
                            message_scores.append((msg, similarity))
                            semantic_found += 1
                            if semantic_found >= 20:  # Limit semantic results
                                break
                    
                    print(f"   âœ… {semantic_found} Ù†ØªÛŒØ¬Ù‡ Ù…Ø¹Ù†Ø§ÛŒÛŒ ÛŒØ§ÙØª Ø´Ø¯")
            
            # Sort by similarity
            message_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Check if we found relevant information (similarity > 0.3 for keyword matches, 0.5 for semantic)
            relevant_messages = [msg for msg, score in message_scores if score > 0.5 or (score > 0.3 and any(word in msg.get('text', '').lower() for word in query.lower().split()))]
            
            result = f"Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¯Ø± Ú©Ø§Ù†Ø§Ù„ {channel_username}:\n\n"
            
            if relevant_messages:
                # Limit to top 15 most relevant messages to avoid overwhelming responses
                top_messages = message_scores[:15]
                result += f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø· ÛŒØ§ÙØª Ø´Ø¯! ({len(top_messages)} Ù¾ÛŒØ§Ù… Ø§Ø² {len(relevant_messages)} Ù¾ÛŒØ§Ù… Ù…Ø±ØªØ¨Ø·)\n\n"
                
                for i, (msg, score) in enumerate(top_messages, 1):
                    text = msg.get('text', '')[:300]  # Show more text
                    date = msg.get('date', 'Ù†Ø§Ù…Ø´Ø®Øµ')
                    msg_id = msg.get('id', 'Ù†Ø§Ù…Ø´Ø®Øµ')
                    channel_username = msg.get('channel', msg.get('channel_name', channel_username))
                    
                    result += f"{i}. Ø§Ù…ØªÛŒØ§Ø²: {score:.3f} | ØªØ§Ø±ÛŒØ®: {date}\n"
                    result += f"   Ù…ØªÙ†: {text}"
                    if len(msg.get('text', '')) > 300:
                        result += "..."
                    result += f"\n   ğŸ”— Ù„ÛŒÙ†Ú©: https://t.me/{channel_username}/{msg_id}\n\n"
                
                # Add note about total results if there are more
                if len(relevant_messages) > 15:
                    result += f"ğŸ“ ØªÙˆØ¬Ù‡: {len(relevant_messages) - 15} Ù¾ÛŒØ§Ù… Ù…Ø±ØªØ¨Ø· Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ² ÛŒØ§ÙØª Ø´Ø¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÚ¯ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯.\n\n"
            else:
                highest_score = message_scores[0][1] if message_scores else 0
                result += f"âŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø· ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {highest_score:.3f})\n\n"
                # Show top 10 for debugging
                for i, (msg, score) in enumerate(message_scores[:10], 1):
                    text = msg.get('text', '')[:200]
                    date = msg.get('date', 'Ù†Ø§Ù…Ø´Ø®Øµ')
                    msg_id = msg.get('id', 'Ù†Ø§Ù…Ø´Ø®Øµ')
                    channel_username = msg.get('channel', msg.get('channel_name', channel_username))
                    
                    result += f"{i}. Ø§Ù…ØªÛŒØ§Ø²: {score:.3f} | ØªØ§Ø±ÛŒØ®: {date}\n"
                    result += f"   Ù…ØªÙ†: {text}"
                    if len(msg.get('text', '')) > 200:
                        result += "..."
                    result += f"\n   ğŸ”— Ù„ÛŒÙ†Ú©: https://t.me/{channel_username}/{msg_id}\n\n"
            
            return result
            
        except Exception as e:
            return f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§: {e}"
    
    def get_channel_info(self, channel_username: str) -> str:
        """Get information about a specific channel"""
        try:
            if channel_username not in self.database['channels']:
                return f"Ú©Ø§Ù†Ø§Ù„ {channel_username} Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª"
            
            channel_data = self.database['channels'][channel_username]
            config_info = next((ch for ch in self.active_channels if ch['username'] == channel_username), None)
            
            result = f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù†Ø§Ù„ {channel_username}:\n"
            result += f"Ù†Ø§Ù…: {channel_data.get('name', 'Ù†Ø§Ù…Ø´Ø®Øµ')}\n"
            result += f"ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§: {len(channel_data.get('messages', []))}\n"
            
            if config_info:
                result += f"ØªÙˆØ¶ÛŒØ­Ø§Øª: {config_info.get('description', 'Ù†Ø§Ù…Ø´Ø®Øµ')}\n"
                result += f"Ø§ÙˆÙ„ÙˆÛŒØª: {config_info.get('priority', 'Ù†Ø§Ù…Ø´Ø®Øµ')}\n"
            
            return result
            
        except Exception as e:
            return f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù†Ø§Ù„: {e}"
    
    def expand_search(self, current_channels: str, query: str) -> str:
        """Expand search to more channels - 5 at a time"""
        try:
            # Parse current channels
            current_list = [ch.strip() for ch in current_channels.split(',')]
            
            # Priority channels that should be checked if not already included
            priority_channels = ['sharifdaily', 'sharif_senfi']
            
            # Find more relevant channels
            query_embedding = self.embedding_model.encode(query)
            
            channel_scores = []
            additional_channels = []
            
            # First, add priority channels if not already checked
            for priority_ch in priority_channels:
                if priority_ch not in current_list and priority_ch in [ch['username'] for ch in self.active_channels]:
                    additional_channels.append(priority_ch)
                    if len(additional_channels) >= 5:
                        return f"Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ (5 ØªØ§): {', '.join(additional_channels)}"
            
            # Then add semantic matches
            for channel in self.active_channels:
                if channel['username'] not in current_list and channel['username'] not in additional_channels:
                    channel_text = f"{channel['name']} {channel['description']}"
                    channel_embedding = self.embedding_model.encode(channel_text)
                    similarity = self.cosine_similarity(query_embedding, channel_embedding)
                    channel_scores.append((channel['username'], similarity))
            
            # Sort by similarity and add top matches
            channel_scores.sort(key=lambda x: x[1], reverse=True)
            
            for username, score in channel_scores:
                if len(additional_channels) < 5:
                    additional_channels.append(username)
                else:
                    break
            
            return f"Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ (5 ØªØ§): {', '.join(additional_channels)}"
            
        except Exception as e:
            return f"Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø³ØªØ±Ø´ Ø¬Ø³ØªØ¬Ùˆ: {e}"
    
    def preprocess_text(self, text: str) -> str:
        """Preprocess text for better semantic search"""
        import re
        
        # Quick check for very short texts
        if len(text) < 10:
            return text
        
        # Remove URLs
        text = re.sub(r'http[s]?://[^\s]+', '', text)
        
        # Remove emojis and special characters but keep Persian text
        text = re.sub(r'[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFFa-zA-Z0-9\s]', ' ', text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Only remove stop words for longer texts
        if len(text) > 50:
            stop_words = ['Ùˆ', 'Ø¯Ø±', 'Ø¨Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ø¨Ø§', 'Ø¨Ø±Ø§ÛŒ', 'ØªØ§', 'Ø±Ø§', 'ÛŒØ§', 'Ø§Ù…Ø§', 'Ø§Ú¯Ø±', 'Ú†ÙˆÙ†', 'Ú†Ø±Ø§', 'Ú†Ú¯ÙˆÙ†Ù‡', 'Ú©Ø¬Ø§', 'Ú©ÛŒ', 'Ú†Ù‡', 'Ú†Ù†Ø¯', 'Ù‡Ù…', 'Ù†ÛŒØ²', 'Ù‡Ù…Ú†Ù†ÛŒÙ†', 'Ù‡Ù…Ù‡', 'Ù‡ÛŒÚ†', 'Ù‡Ø±']
            words = text.split()
            words = [word for word in words if word not in stop_words and len(word) > 1]
            return ' '.join(words)
        
        return text
    
    def cosine_similarity(self, vec1, vec2):
        """Calculate cosine similarity between two vectors"""
        import numpy as np
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return dot_product / (norm1 * norm2)
    
    def load_database(self) -> Dict[str, Any]:
        """Load the local database"""
        try:
            with open(self.database_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"âœ… Loaded database with {data['metadata']['total_channels']} channels and {data['metadata']['total_messages']} messages")
                return data
        except FileNotFoundError:
            print(f"âš ï¸ Database file {self.database_file} not found!")
            return {"metadata": {"total_channels": 0, "total_messages": 0}, "channels": {}}
        except Exception as e:
            print(f"âŒ Error loading database: {e}")
            return {"metadata": {"total_channels": 0, "total_messages": 0}, "channels": {}}
    
    def load_config(self, config_file: str = None) -> Dict[str, Any]:
        """Load configuration from JSON file"""
        if config_file is None:
            config_file = self.config_file
            
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"Config file {config_file} not found. Using default config.")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict[str, Any]:
        """Get default configuration"""
        return {
            "channels": [],
            "llm_settings": {
                "model": "gemini-2.5-flash",
                "temperature": 0.7
            }
        }
    
    def query(self, question: str) -> str:
        """Main query function using LangChain agent"""
        try:
            if not self.agent:
                return "âŒ LangChain system not initialized. Please check dependencies."
            
            print(f"ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ: {question}")
            print("ğŸ¤– Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LangChain Agent...")
            
            # Run agent
            result = self.agent.invoke({"input": question})
            
            return result.get('output', 'Ù¾Ø§Ø³Ø® ÛŒØ§ÙØª Ù†Ø´Ø¯')
            
        except Exception as e:
            print(f"âŒ Error in LangChain query: {e}")
            return f"Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ø±Ø® Ø¯Ø§Ø¯: {e}"

def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LangChain RAG System")
    parser.add_argument("--daemon", action="store_true", help="Run as daemon")
    parser.add_argument("--query", type=str, help="Single query")
    
    args = parser.parse_args()
    
    rag = LangChainRAGSystem()
    
    print("ğŸ¤– Ø³ÛŒØ³ØªÙ… LangChain RAG Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!")
    print("ğŸ’¾ Ø¯ÛŒØªØ§Ø¨ÛŒØ³:", rag.database_file)
    print("ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§:", rag.database['metadata'].get('total_channels', 0))
    print("ğŸ’¬ ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§:", rag.database['metadata'].get('total_messages', 0))
    print()
    
    if args.query:
        # Single query
        result = rag.query(args.query)
        print("\n" + "="*60 + "\n")
        print("ğŸ¤– Ù¾Ø§Ø³Ø®:")
        print(result)
        return
    
    if args.daemon:
        # Interactive mode
        print("ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… LangChain RAG...")
        print("ğŸ”„ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ctrl+C Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯")
        print()
        
        while True:
            try:
                query = input("â“ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯ (ÛŒØ§ 'quit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬): ")
                if query.lower() in ['quit', 'exit', 'Ø®Ø±ÙˆØ¬']:
                    break
                
                if query.strip():
                    print("\n" + "="*50)
                    result = rag.query(query)
                    print("\nğŸ¤– Ù¾Ø§Ø³Ø®:")
                    print(result)
                    print("="*50 + "\n")
                    
            except KeyboardInterrupt:
                print("\nğŸ›‘ Ø®Ø±ÙˆØ¬ Ø§Ø² Ø³ÛŒØ³ØªÙ…...")
                break
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§: {e}")
    
    # Default interactive mode
    while True:
        try:
            query = input("â“ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯ (ÛŒØ§ 'quit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬): ")
            if query.lower() in ['quit', 'exit', 'Ø®Ø±ÙˆØ¬']:
                break
            
            if query.strip():
                print("\n" + "="*50)
                result = rag.query(query)
                print("\nğŸ¤– Ù¾Ø§Ø³Ø®:")
                print(result)
                print("="*50 + "\n")
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ Ø®Ø±ÙˆØ¬ Ø§Ø² Ø³ÛŒØ³ØªÙ…...")
            break
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§: {e}")

if __name__ == "__main__":
    main() 